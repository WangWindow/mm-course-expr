# 实验一 一维PCA和二维PCA，识别特征脸

## 一、实验目的

- 理解主成分分析（PCA）和二维主成分分析（2D-PCA）的基本思想与数学原理，掌握其在降维与特征提取中的作用。
- 在人脸识别任务中，学习如何利用一维PCA构建“特征脸（Eigenfaces）”并进行分类识别。
- 理解2D-PCA相对于传统PCA在图像表示上的改进思路，比较两种方法在特征表达和识别效果上的差异。
- 熟悉在 Python 环境下使用 `numpy`、`matplotlib` 等库进行矩阵运算、特征分解与可视化的基本流程。

## 二、实验原理

### 1. 一维PCA与特征脸

PCA 的目标是在保持数据主要变化信息的前提下，将高维数据投影到低维子空间。设原始样本为 $x_i \in \mathbb{R}^d$，PCA 的基本步骤：

1. 计算样本均值向量 $\mu = \frac{1}{N}\sum_{i=1}^N x_i$，对样本做中心化处理：$\tilde{x}_i = x_i - \mu$。
2. 计算协方差矩阵 $S = \frac{1}{N}\sum_{i=1}^N \tilde{x}_i \tilde{x}_i^T$。
3. 对 $S$ 做特征值分解，得到特征值 $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_d$ 及对应特征向量 $u_1, u_2, \dots, u_d$。
4. 取前 $k$ 个最大特征值对应的特征向量构成投影矩阵 $U_k = [u_1, u_2, \dots, u_k]$，将样本映射到低维空间：
	$$y_i = U_k^T (x_i - \mu).$$

在“特征脸”方法中，我们将每张人脸图像按行优先方式拉平成一维向量作为 $x_i$，得到的特征向量 $u_j$ 再按图像尺寸重排后就是一张“特征脸”。特征脸在视觉上体现了数据集中最具有代表性的变化模式。识别时，将待测人脸投影到特征脸子空间中，与训练集中已知类别的投影向量做距离度量（如欧式距离），根据最近邻原则完成分类。

### 2. 二维PCA（2D-PCA）

传统PCA需要将图像矩阵展平成长向量，不仅维度高、计算协方差矩阵开销大，而且破坏了图像的二维结构信息。2D-PCA 直接在图像矩阵上进行特征提取。设每个样本图像为矩阵 $A_i \in \mathbb{R}^{m\times n}$，2D-PCA 的基本思想是寻找一个投影矩阵 $W \in \mathbb{R}^{n\times d}$，使得投影后的特征矩阵 $Y_i = A_i W$ 能最大程度保留样本之间的散布：

1. 计算样本均值矩阵 $\bar{A} = \frac{1}{N}\sum_{i=1}^N A_i$。
2. 构造图像协方差矩阵：
	$$G = \frac{1}{N}\sum_{i=1}^N (A_i - \bar{A})^T (A_i - \bar{A}).$$
3. 对矩阵 $G$ 进行特征值分解，选取最大若干特征值对应的特征向量构成投影矩阵 $W$。
4. 对每个样本，计算 $Y_i = A_i W$ 作为其二维特征表示，后续再将 $Y_i$ 展平或使用矩阵距离进行分类。

与一维PCA相比，2D-PCA 直接在矩阵空间上建模，协方差矩阵维度更低，计算量更小，而且保留了图像行方向的结构信息，通常在样本数有限时表现出更稳定的特征提取能力。

### 3. 特征脸识别流程概述

综合以上原理，本实验中的特征脸识别过程可概括为：

1. 数据预处理：读取人脸图像，统一尺寸并灰度化，将像素值标准化到 [0,1] 或 [0,255] 范围。
2. 训练阶段：
	- 计算训练样本的人脸均值图像；
	- 计算协方差矩阵并进行特征分解；
	- 选取前 $k$ 个主成分，得到特征脸集合；
	- 将所有训练样本投影到特征空间，保存其特征表示及标签。
3. 测试识别：
	- 将待测人脸做同样的预处理和投影；
	- 计算其与训练样本特征向量之间的距离；
	- 选取距离最近的类别作为识别结果。

## 三、实验环境与数据说明

- 硬件环境：
  - CPU：常规多核处理器即可完成实验；
  - 内存：≥ 8 GB。
- 软件环境：
  - 操作系统：Windows 10；
  - 编程语言：Python 3（使用项目根目录的 `pyproject.toml` 中配置的环境）；
  - 主要第三方库：`numpy`、`matplotlib` 等。
- 开发工具：Visual Studio Code，配合 Jupyter Notebook 进行实验与可视化。
- 数据说明：
  - 实验数据存放于项目根目录的 `data/olivetti_py3.pkz` 文件中，基于 Olivetti faces 数据集预处理而来；
  - 每个样本是一张尺寸固定的人脸灰度图像，标签为对应的人物编号；
  - 在 Notebook `expr1/pca.ipynb` 中用于一维PCA与特征脸实验，在 `expr1/2d-pca.ipynb` 中用于2D-PCA实验。

## 四、实验步骤与结果

### 1. 一维PCA与特征脸

1. **加载数据与可视化样本**：
	- 使用 `numpy` 从 `olivetti_py3.pkz` 中读取人脸图像矩阵和对应标签；
	- 随机选取若干张人脸图像，使用 `matplotlib` 绘制，观察数据的大致分布和姿态变化。

2. **构建训练集与测试集**：
	- 按人物编号划分训练和测试样本，例如每个身份选取若干张用于训练，其余用于测试；
	- 将每张图像拉平成向量，构成训练数据矩阵 $X \in \mathbb{R}^{d \times N}$。

3. **计算均值脸与特征脸**：
	- 计算所有训练样本的平均向量并重塑为图像形式，得到“均值脸”，可以使用 `imshow` 绘制；
	- 基于中心化后的训练数据计算协方差矩阵并进行特征值分解；
	- 选取前 $k$ 个主成分，将其重构为图像形式并可视化，得到一组具有代表性纹理和结构的特征脸。

4. **投影与重构实验**：
	- 将训练和测试样本投影到特征脸子空间，得到低维特征向量；
	- 通过保留不同数量的主成分（例如 $k=10, 20, 50$），对样本进行重构，观察重构图像与原图之间的差异，分析信息损失情况。

5. **人脸识别实验**：
	- 在特征空间中使用简单的最近邻分类器，对测试集人脸进行识别；
	- 统计识别准确率，并观察若干识别正确与识别错误的样例图像，分析误差来源。

整体上，在 `pca.ipynb` 中可以看到：随着保留主成分数量 $k$ 的增加，重构图像质量逐步提升，识别率也有一定改善；但当 $k$ 过大时会带来过拟合风险和计算开销增加。

### 2. 二维PCA（2D-PCA）

1. **直接在图像矩阵上建模**：
	- 不再将图像展平为长向量，而是保持原始矩阵形状 $m \times n$；
	- 计算均值图像矩阵 $\bar{A}$，并据此构造 2D-PCA 的协方差矩阵 $G$。

2. **求取投影矩阵并提取特征**：
	- 对 $G$ 进行特征值分解，选取前 $d$ 个特征向量组成投影矩阵 $W$；
	- 对每个样本计算 $Y_i = A_i W$，得到尺寸为 $m \times d$ 的特征矩阵。

3. **可视化与重构观察**：
	- 对若干样本的二维特征进行可视化（例如展示特征矩阵的灰度图或主方向）；
	- 将特征矩阵再映射回近似原图像（如果在 Notebook 中实现了相应过程），比较重构效果。

4. **2D-PCA 人脸识别**：
	- 将 $Y_i$ 展平为特征向量，使用同样的最近邻方法进行分类；
	- 记录 2D-PCA 在相同训练/测试划分下的识别准确率，与一维PCA结果对比。

实验现象表明：在相近特征维度下，2D-PCA 通常能够在较少主成分数量时获得与 PCA 相当或略优的识别效果，同时计算协方差矩阵与特征分解的时间开销更小。

## 五、实验结果分析

1. **降维效果与信息保留**：
	- 一维PCA通过特征值大小描述每个主成分对整体方差的贡献，可通过累计贡献率曲线选择合适的主成分数 $k$；
	- 在实验中，前几十个主成分往往已经可以重构出较为清晰的人脸轮廓，说明人脸数据在高维空间中具有较强的低维结构。

2. **PCA 与 2D-PCA 的对比**：
	- 传统PCA需要在非常高维的空间中计算协方差矩阵，而样本数量有限时容易出现协方差矩阵病态、特征估计不稳定等问题；
	- 2D-PCA 直接基于图像矩阵构造协方差，矩阵维度显著降低，更适合样本数与图像维度同级甚至更小时的场景；
	- 在本实验中，2D-PCA 在相同特征维度下通常取得了与 PCA 相近甚至更高的识别率，同时计算时间更短、数值更稳定。

3. **识别误差原因分析**：
	- 低维特征维度过小会导致关键信息丢失，重构图像模糊、个体差异减弱，进而降低识别准确率；
	- 数据集中存在姿态变化、光照变化、表情变化等因素，这些变化在主成分空间中有时会被解释为“主要变化”，从而干扰身份特征；
	- 使用最近邻分类时，对噪声和类间边界分布较为敏感，如训练样本不足或分布不均衡，也会造成误识；
	- 仅使用线性投影的 PCA/2D-PCA 难以刻画高度非线性的特征结构，限制了上限性能。

4. **与论文结论的对照**：
	- 参考《Eigenfaces for Recognition》中提出的特征脸方法，本实验复现了“利用少量主成分即可实现较高识别率”的基本结论；
	- 对比《Two-dimensional PCA: A new approach to appearance-based face representation and recognition》中对2D-PCA的分析，实验也体现出 2D-PCA 在图像表示上的紧凑性和鲁棒性优势。

## 六、心得体会

通过本次实验，我对 PCA 和 2D-PCA 在人脸识别中的应用有了更加直观的理解。一方面，从实现角度体会到：

- 仅通过 `numpy` 的基础线性代数运算，就可以完成协方差矩阵的构造与特征值分解，说明许多经典模式识别算法的核心实现并不复杂；
- 在 Notebook 中将均值脸、特征脸以及重构结果可视化，可以非常直观地感受到“降维”到底丢掉了哪些信息，有助于理解主成分的物理意义；
- 通过调节主成分个数、训练测试划分等超参数，可以清楚地看到模型容量、过拟合与泛化性能之间的权衡关系。

另一方面，从方法比较的角度：

- 2D-PCA 在保持图像二维结构、降低协方差矩阵维度方面的优势非常明显，在小样本场景下更容易得到稳定的特征；
- 实验中也发现，单纯依赖线性投影的 PCA/2D-PCA 虽然实现简单，但在复杂光照、姿态变化面前能力有限，如果要在真实应用中获得更高的识别率，还需要结合后续更高级的方法（例如 Fisher 判别分析、核方法或深度学习特征）。

总体而言，本实验让我更加清楚地认识到：数学上的线性代数和概率统计知识是理解多媒体分析与模式识别算法的基础；通过亲手实现和调参，可以把抽象的公式和真实的数据联系起来，为后续学习更复杂的特征学习和深度学习模型打下良好基础。


# 实验二 基于傅里叶描述子的手写数字识别

## 一、实验目的

- 理解傅里叶描述子（Fourier Descriptors）在形状表示与轮廓特征提取中的基本思想。
- 掌握从手写数字图像中提取轮廓、构造复数信号并计算傅里叶谱的完整流程。
- 学习如何将傅里叶描述子作为特征输入到传统机器学习分类器（如逻辑回归）中，实现多类别数字识别。
- 通过实验观察频谱特征与分类效果之间的关系，体会特征设计、归一化与分类模型选择对识别性能的影响。

## 二、实验原理

### 1. 轮廓表示与傅里叶描述子

对于二值图像中的连通区域，可以提取目标的外轮廓，将其表示为按顺序排列的一系列边界点：

$$C = \{(x_k, y_k) \mid k = 0, 1, \dots, N-1\}.$$

为了便于频域分析，我们通常将轮廓坐标表示为复数序列：

$$z_k = x_k + j y_k, \quad k = 0, 1, \dots, N-1.$$

对该复数序列进行离散傅里叶变换（DFT），得到频域系数：

$$Z_n = \sum_{k=0}^{N-1} z_k e^{-j 2\pi nk / N}, \quad n = 0, 1, \dots, N-1.$$

这些傅里叶系数 $Z_n$ 即为傅里叶描述子，它们从频域角度刻画了轮廓形状的全局与局部特征。低频系数描述轮廓的整体形状，高频系数刻画细节与噪声。

### 2. 归一化与不变性

在实际应用中，我们希望特征对平移、尺度和起始点选择等变化具有一定的不变性。因此，常见的归一化处理包括：

- **平移不变**：通过减去轮廓重心，或直接将直流分量 $Z_0$ 置零；
- **尺度不变**：对所有傅里叶系数按某种范数进行归一化，例如除以特征向量的 L2 范数；
- **起始点不敏感**：轮廓起点的改变对应频谱的整体相位变化，可通过只使用幅度信息或适当的系数选取来减弱影响。

在本实验的实现中，首先将 $Z_0$ 置为 0，以去除平移影响；然后取前若干个非零频率的实部与虚部拼接成特征向量，并通过 L2 归一化实现尺度标准化。

### 3. 手写数字识别流程

综合 Notebook 代码，本实验的整体识别流程如下：

1. **数据准备**：从 `sklearn.datasets` 加载 `digits` 手写数字数据集，将 8×8 灰度图作为基础样本；
2. **图像预处理与轮廓提取**：
	- 将原始图像缩放到固定大小（如 64×64），并归一化为浮点图；
	- 使用大津法（Otsu）阈值分割得到二值图像；
	- 通过形态学闭运算填补轮廓细小空洞，提升边界连通性；
	- 使用 `find_contours` 提取目标轮廓，并选取其中最长的轮廓作为数字主体轮廓；
3. **傅里叶描述子提取**：
	- 对轮廓按照弧长进行重采样，使其包含固定数量的等距点，保证不同图像间具有可比性；
	- 将采样点坐标构造成复数序列并进行 FFT；
	- 舍弃直流分量，选取前 `NUM_DESCRIPTORS` 个频率系数的实部与虚部拼接成特征向量；
	- 对特征向量做 L2 归一化，得到最终的傅里叶描述子特征。
4. **分类与评估**：
	- 使用 `train_test_split` 将样本划分为训练集和测试集，保证各类数字分布均衡；
	- 构建包含 `StandardScaler` 与 `LogisticRegression` 的管线，对特征进行标准化后训练多项逻辑回归分类器；
	- 在测试集上评估分类准确率，并展示部分典型识别结果。

## 三、实验步骤与内容

### 1. 环境配置与库导入

在 Notebook 的开头，首先导入所需的第三方库：

- `numpy`：用于数值计算与向量/矩阵操作；
- `matplotlib`：用于可视化图像与频谱；
- `sklearn`：提供 `load_digits` 数据集、数据划分工具以及逻辑回归模型；
- `skimage`：用于图像缩放、阈值分割、形态学处理与轮廓提取；
- 以及 `pathlib`、`PIL` 等用于路径和图片读写的工具。

同时配置 `matplotlib` 支持中文显示，以便在图像标题中使用中文说明。

### 2. 辅助函数实现

Notebook 中实现了两个关键辅助函数：

1. **`resample_contour`**：
	- 输入一条轮廓点序列和目标点数 `num_points`；
	- 计算相邻点之间的欧氏距离，得到曲线长度累计数组；
	- 通过一维插值，在等弧长位置重新采样出固定数量的点；
	- 若轮廓点数过少或总长度为 0，则重复首点以保证输出维度稳定。

2. **`extract_fourier_descriptors`**：
	- 接收单张灰度图像，先缩放到统一大小并进行 Otsu 阈值分割与形态学闭运算；
	- 使用 `find_contours` 提取二值图像的轮廓，若未检测到轮廓则返回零特征；
	- 选取最长轮廓并调用 `resample_contour` 进行重采样；
	- 将轮廓坐标转换为复数序列并计算 FFT；
	- 置零直流分量，截取前 `num_descriptors` 个频率系数的实部与虚部合并；
	- 对最终特征向量做 L2 归一化，返回定长特征。

### 3. 加载数据集与单样本分析

1. 使用 `load_digits()` 加载手写数字数据集，得到图像数组 `images` 和标签 `labels`；
2. 选取一个样本（例如索引为 0 的数字），调用 `extract_fourier_descriptors` 提取其描述子；
3. 计算描述子的幅度谱并绘制：左图展示原始数字图像，右图展示傅里叶描述子幅度随频率索引变化的曲线，以直观观察低频与高频成分的分布。

### 4. 批量提取特征

对整个 `digits` 数据集中的所有图像逐一调用 `extract_fourier_descriptors`，将结果堆叠成特征矩阵 `features`，其形状为：

- 样本数 × 特征维度（其中特征维度为 `2 * NUM_DESCRIPTORS`）。

通过打印形状确认特征维度设置是否合理、提取过程是否成功。

### 5. 训练分类器并评估

1. 使用 `train_test_split` 按比例（如 3:1）划分训练集和测试集，并使用 `stratify` 参数保证各类别样本数大致均衡；
2. 构造包含 `StandardScaler` 和 `LogisticRegression` 的 `Pipeline`，对训练集特征进行标准化并训练多项逻辑回归模型；
3. 在测试集上调用 `score` 方法计算分类准确率，并打印测试集整体识别率（Notebook 中输出类似 `测试集准确率: 0.9xxx` 的结果）；
4. 进一步在测试集中为每个数字类别选取一个代表样本，显示其原始图像，并在标题中标注“真实标签 | 预测标签”，用于直观检查分类效果和错误样本。

### 6. 外部图片识别接口

为了便于后续扩展，Notebook 中还定义了 `predict_digit_from_path` 函数：

- 从给定的图片路径读取手写数字图像，并转换为灰度；
- 调用与数据集中相同的 `extract_fourier_descriptors` 过程提取描述子；
- 使用训练好的 `pipeline` 模型预测该图片对应的数字类别；
- 返回预测结果，实现对外部手写数字图片的快速识别接口。

## 四、实验结果与现象

1. **傅里叶描述子幅度谱**：
	- 对单个样本数字的实验显示，傅里叶描述子的幅度谱在低频部分能量集中，而高频部分幅度较小；
	- 这与“轮廓整体形状由低频成分主导、细节与噪声由高频成分描述”的理论相吻合，也说明在特征压缩时可以仅保留少量低频系数。

2. **整体识别率**：
	- 在当前参数设置下（例如 `NUM_DESCRIPTORS = 32`，逻辑回归最大迭代次数 1000），测试集准确率达到较高水平（约 90% 以上）；
	- 说明利用傅里叶描述子作为轮廓特征，加上简单的线性分类器，就可以在标准手写数字数据集上获得不错的识别效果。

3. **典型识别样例**：
	- Notebook 中展示了测试集中 0–9 各类数字的典型识别结果，大部分样本可以被正确分类；
	- 对于易混淆的数字（如 3/5、4/9 等），在轮廓形状较为相似或书写风格特别时，偶尔会出现误判；
	- 这些现象反映出单一轮廓特征在某些细节区分上存在局限，可能需要结合笔画内部纹理或多尺度特征来进一步提升性能。

4. **参数与预处理的影响**：
	- 轮廓重采样点数、保留的描述子数量以及阈值分割和形态学操作的设置，都会影响轮廓质量与特征稳定性；
	- 适当增加描述子数量可以提升对复杂形状的表征能力，但过多会引入噪声并增加模型复杂度；
	- 标准化（`StandardScaler`）对逻辑回归训练的收敛速度和最终识别率也有明显帮助。

## 五、总结和展望

通过本实验，我对“基于轮廓的形状特征”和“傅里叶描述子”的概念有了更加直观的认识。从实现过程和结果上，可以得到如下几点体会：

1. **傅里叶描述子适合表示整体形状**：
	- 通过将轮廓点映射为复数序列并进行 FFT，可以用有限个低频系数有效地描述手写数字的大致形状；
	- 合理的归一化处理（去除直流分量、归一化幅度）使得特征对平移和尺度变化具有一定鲁棒性。

2. **特征 + 经典机器学习仍然有效**：
	- 即便不使用深度学习，仅依靠设计良好的特征（傅里叶描述子）和传统分类器（逻辑回归），在标准数据集上也能取得较高的识别率；
	- 这也说明在样本规模较小、计算资源有限的场景下，经典特征工程方法依然具有实用价值。

3. **局限性与改进方向**：
	- 仅使用轮廓特征难以区分内部结构差别较大的数字，且对书写断笔、噪声、轮廓破损较为敏感；
	- 可以考虑结合区域特征（如灰度纹理、方向梯度直方图）、多尺度轮廓描述子或更为复杂的分类器（如 SVM、随机森林）以进一步提升性能；
	- 在更大规模、更复杂的数据集上，可以将傅里叶描述子作为先验或辅助特征，与卷积神经网络等深度模型结合，实现精度与效率的平衡。

总体来看，本实验帮助我系统梳理了从图像预处理、轮廓提取、特征构造到模型训练和评估的完整流程，加深了对“频域特征”和“形状识别”之间联系的理解，为后续学习形状分析与图像理解的高级方法打下了基础。


# 实验三 AI 短视频制作

## 一、实验基本信息

- 实验主题：基于大模型与多媒体编辑工具的新闻类 AI 短视频制作。
- 实验内容：围绕“英伟达 50 亿美元战略投资英特尔”这一新闻事件，利用多种 AI 工具完成文案撰写、画面生成、视频合成与剪辑，最终输出一条结构完整的短视频。
- 使用工具：
  - 文本与提示词生成：通义千问；
  - 图片生成：豆包（根据提示词生成图片）；
  - 文生视频/图文生成视频：sora2（根据图片与场景描述生成视频片段）；
  - 视频剪辑与整合：必剪（负责片段拼接、转场、字幕与导出）。
- 实验素材：
  - 文案素材来源于 `expr3/要求.txt` 中给出的新闻内容；
  - 图片需求与画面规划参考 `expr3/prompts.txt` 中的提示；
  - 实验过程中生成的图片和视频片段保存在 `expr3/images/` 等目录下，作为最终视频的构成元素。

## 二、实验原理

本实验聚焦于“多模态大模型 + 传统剪辑工具”的协同工作流程，其核心原理可概括为以下几个方面：

1. **大模型驱动的内容生成与提示词工程**：
	- 通义千问作为通用大模型，具备较强的自然语言理解与生成能力，可根据给定新闻材料自动生成更适合短视频叙事的分镜文案与图片/视频提示词；
	- 通过“提示词工程”（Prompt Engineering），可以控制生成内容的风格（科技感、金融感）、构图元素（logo、股价曲线、握手画面等）和色调基调（冷色调、蓝绿色等），从而提高后续图像生成的可控性与一致性。

2. **图像生成模型（豆包）与视觉叙事**：
	- 豆包根据文本提示词生成静态图片，是整个视频视觉素材的主要来源之一；
	- 通过将“nvidia 与 intel 的商标”“股价暴涨的 K 线/箭头”“两家企业合作握手”等概念抽象成清晰的提示词，即可得到多个候选画面，用于构成视频的关键帧或背景。

3. **文图到视频的跨模态生成（sora2）**：
	- sora2 支持将图片与简要场景描述作为输入，自动生成包含镜头运动、光影变化和过渡效果的短视频片段；
	- 可以理解为在静态图像的基础上进行时间维度上的“扩展”，通过补全中间帧、模拟镜头推拉摇移等方式，使画面更加生动；
	- 通过控制输入文本（如“缓慢推进镜头”“从左至右的平移”“背景出现股价曲线动态上扬”）可以一定程度上引导视频风格。

4. **传统非线性剪辑软件（必剪）的整合作用**：
	- 虽然 AI 可以自动生成文字、图片与视频片段，但最终成片仍然需要通过剪辑软件进行时间轴上的精细编排；
	- 必剪提供时间线编辑、音视频轨道管理、转场特效、字幕与背景音乐等功能，是将各个 AI 生成片段“串联成故事”的关键工具；
	- 将 AI 生成与人工剪辑结合，可以兼顾效率与可控性：AI 负责“生产素材”，必剪负责“结构和节奏”。

综上，实验本质上是一次“AI+工具链”协同的工作流设计实验，重点在于如何将不同工具的能力串联起来，完成从文本 → 图片 → 视频 → 成片的多阶段转换。

## 三、实验步骤（含具体提示词与参数）

本实验以“英伟达战略投资英特尔”为主线，参考 `prompts.txt` 中的三部分内容，将短视频分为三个画面段落。典型的操作步骤如下。

### 1. 使用通义千问生成结构化文案与提示词

1. **输入原始新闻材料**：
	- 将 `要求.txt` 中的内容复制到通义千问，要求其根据新闻事件生成适合 30–60 秒短视频的解说文案，并分为三段，对应：
	  1. 合作事件的宣布；
	  2. 股价变化与市场反应；
	  3. 合作意义与未来展望。
2. **生成画面提示词**：
	- 在通义千问中额外要求：针对每一段文案，生成 2–3 条用于图片生成的英文/中英文混合提示词，明确包含以下要素：
	  - 第一部分：NVIDIA 与 Intel 的 logo 同屏、高科技背景；
	  - 第二部分：股价上涨的 K 线图、绿色上涨箭头、市场行情屏幕；
	  - 第三部分：在两家 logo 下握手、合作、芯片与数据中心等元素；
	- 示例提示词（用于豆包）：
	  - “NVIDIA and Intel logos side by side, futuristic blue tech background, high contrast, 16:9”
	  - “Stock market chart with Intel stock price soaring 30%, green upward arrows, financial news style”
	  - “Two business people shaking hands under NVIDIA and Intel logos, symbolizing strategic partnership, cinematic lighting”。

### 2. 使用豆包生成图片素材

1. **按画面部分依次生成图片**：
	- 将通义千问给出的提示词逐条输入豆包，设置统一的画幅比例（例如 16:9 或 9:16，视目标平台而定）；
	- 每个部分生成 2–3 张候选图像，保证画面风格尽量统一（可以在提示词中固定“blue tech style”“flat illustration”或“3D render style”等描述）。
2. **保存与筛选**：
	- 将生成的图片按场景归类存入 `expr3/images/` 目录，例如：
	  - `images/part1_logo_01.png`, `images/part1_logo_02.png`；
	  - `images/part2_stock_01.png`, `images/part2_stock_02.png`；
	  - `images/part3_handshake_01.png`, `images/part3_handshake_02.png`；
	- 对比不同图片的构图、清晰度与风格一致性，选出最适合作为关键画面的 1–2 张图。

### 3. 使用 sora2 从图片与文本生成视频片段

1. **设定视频片段时长与风格**：
	- 在 sora2 中，新建 3 个片段项目，分别对应视频的三个章节；
	- 每个片段时长可设置为 6–10 秒，整体控制在 20–30 秒以内；
	- 统一选择科技感较强的风格，如冷色调灯光、细微粒子效果、缓慢镜头运动等。
2. **为每一部分提供图片与场景描述**：
	- 片段一（合作宣布）：
	  - 图片：选择 `images/part1_logo_xx.png`；
	  - 文本提示示例：
		 - “Slow zoom in on NVIDIA and Intel logos side by side, futuristic tech background, slight camera movement, subtle light flares”。
	- 片段二（股价上涨）：
	  - 图片：选择 `images/part2_stock_xx.png`；
	  - 文本提示示例：
		 - “Animated stock chart with Intel price soaring above 30%, dynamic green arrows rising, quick but smooth camera pan, financial news overlay style”。
	- 片段三（合作意义）：
	  - 图片：选择 `images/part3_handshake_xx.png`；
	  - 文本提示示例：
		 - “Two business silhouettes shaking hands under NVIDIA and Intel logos, background showing data centers and chips, slow cinematic camera move, hopeful lighting”。
3. **生成与导出视频片段**：
	- 等待 sora2 渲染完成后，检查画面是否与文案语气匹配（科技、财经、合作氛围）；
	- 若镜头运动过快/过慢，可通过调整提示词（slowly/quickly，cinematic/steady）重新生成；
	- 将满意的片段以 1080p 或平台推荐分辨率导出，命名为 `part1.mp4`, `part2.mp4`, `part3.mp4` 等。

### 4. 在必剪中进行剪辑与整合

1. **导入素材**：
	- 打开必剪，新建工程并设置视频分辨率和帧率（如 1920×1080, 30fps）；
	- 导入 sora2 生成的各段视频片段，以及背景音乐（若有）和配音音频（可以由任意 TTS 工具根据文案合成）。
2. **时间线编排**：
	- 将 `part1.mp4`、`part2.mp4`、`part3.mp4` 按顺序拖入视频轨道，并根据文案长度微调每段时长；
	- 在片段之间添加简单转场（如淡入淡出、交叉溶解），避免段落切换过于生硬。
3. **字幕与文字信息**：
	- 在关键画面上方添加简短文字，例如：
	  - “英伟达 50 亿美元战略投资英特尔”；
	  - “英特尔股价盘前暴涨超 30%”；
	  - “从竞争走向深度绑定的全新关系”；
	- 可以使用必剪的自动字幕功能导入配音音频并生成字幕，再进行人工校对与排版。
4. **音效与导出**：
	- 选择与科技财经主题相适配的轻电子背景音乐，控制音量不压制解说；
	- 最后检查整体节奏、画面与文案是否同步，确认无明显卡顿或画质问题后以 MP4 格式导出成片。

## 四、实验结果

1. **最终成片效果**：
	- 视频整体时长控制在 20–40 秒之间，包含开场 logo 画面、股价上涨画面以及合作握手与未来展望画面三大段落；
	- 画面风格较为统一，统一采用蓝绿科技色调和简洁的信息图表，使观众能快速抓住“合作 + 股价 + 未来”的核心信息；
	- 文案与画面内容对应清晰：第一段聚焦“宣布合作”，第二段强化“股价反应”，第三段总结“深度绑定与未来趋势”。

2. **工具协同表现**：
	- 通义千问在生成分镜文案与提示词方面大幅减少了手动构思时间，能快速得到多种风格备选；
	- 豆包生成的图片清晰度与构图质量较好，并且通过统一提示词可以在一定程度上保持画面风格一致性；
	- sora2 在将静态图转为动态视频方面效果自然，镜头运动与光影变化提升了整体观看感；
	- 必剪将上述素材有效整合，并通过转场、字幕与配音进一步提升了成片的专业感。

## 五、实验总结与分析

1. **对 AI 工具链的整体认识**：
	- 本实验展示了“通义千问 → 豆包 → sora2 → 必剪”的完整 AI 视频制作流程，从文本到图像再到视频，体现了多模态大模型在内容生产中的协同能力；
	- 相比传统完全手工制作的流程，AI 可以在素材生成阶段显著提升效率，使创作者更专注于结构设计与节奏把控。

2. **优点分析**：
	- **效率高**：大部分视觉素材由 AI 生成，减少了手动绘制或拍摄的成本；
	- **易于迭代**：不满意的文案、图片或视频片段可以通过修改提示词快速重新生成；
	- **风格可控**：通过在提示词中控制色调、构图与风格标签，能够在一定范围内保证整体视觉的一致性。

3. **存在的问题与局限**：
	- **细节可控性有限**：有时 AI 生成的画面细节与需求不完全一致，需要多次尝试或在必剪中通过裁剪遮挡来修正；
	- **品牌与版权问题**：在涉及真实品牌 logo（如 NVIDIA 和 Intel）时，需要注意商标使用的合规与平台审核规定；
	- **生成稳定性差异**：不同模型对同一提示词的响应存在随机性，有时会出现风格漂移或不符合预期的结果，需要人工筛选。

4. **改进与展望**：
	- 后续可以尝试引入更精细的“故事板”设计，在通义千问中直接生成带时间标注的分镜脚本，再交由豆包和 sora2 分段制作；
	- 可将 TTS（文本转语音）与配音风格选择纳入统一流程，以进一步自动化解说音轨的生成；
	- 在更复杂的项目中，可以考虑使用更多轨道和复杂转场、特效，使 AI 生成的视频更接近专业级商业作品；
	- 随着多模态大模型能力的增强，有望将文案撰写、画面生成与剪辑决策进一步整合，形成“一键生成+人工微调”的视频生产范式。

通过本实验，我对如何将不同 AI 工具和传统剪辑软件组合使用有了系统性的理解，也体会到提示词设计、素材筛选和节奏把控在 AI 视频制作中的重要性，为后续制作更复杂的多媒体项目打下了实践基础。
