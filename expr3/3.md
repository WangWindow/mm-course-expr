# 实验三 AI 短视频制作实验报告

## 一、实验基本信息

**实验名称**：AI 短视频制作
**实验日期**：2025年11月21日
**实验环境**：Windows 11, VS Code, 豆包 (Doubao), 通义千问 (Qwen), Sora 2, 必剪 (Bcut)

### 1.1 实验背景
随着人工智能（Artificial Intelligence, AI）技术的飞速发展，特别是生成式人工智能（AIGC）的爆发，内容创作领域正在经历一场前所未有的变革。从最初的文本生成（如 ChatGPT），到图像生成（如 Midjourney, Stable Diffusion），再到如今备受瞩目的视频生成（如 Sora, Runway），AI 正在逐步重塑媒体内容的生产方式。

短视频作为当前互联网最主流的信息传播载体，其制作流程通常包括选题、脚本撰写、素材拍摄/绘制、配音、剪辑等多个环节。传统制作方式不仅周期长、成本高，而且对创作者的专业技能有较高要求。然而，AI 技术的介入使得“一个人就是一支队伍”成为可能。通过 AI 工具，创作者可以快速生成高质量的文案、图片、视频片段以及配音，极大地降低了短视频制作的门槛，提高了生产效率。

本次实验旨在探索利用多种 AI 工具协同完成一个完整的新闻类短视频的制作流程。实验选取了一则关于“英伟达战略投资英特尔”的虚构科技新闻作为题材，通过 AI 模型生成视觉素材、编写分镜脚本、生成动态视频，并最终通过剪辑软件进行合成与后期处理。这不仅是对 AI 工具生成能力的测试，更是对未来媒体内容生产工作流的一次前瞻性实践。

### 1.2 实验目的
1.  **掌握 AIGC 工具的使用**：熟悉并掌握当前主流的 AI 生成工具（如豆包、通义千问、Sora 2）的操作方法和提示词（Prompt）技巧。
2.  **理解多模态生成原理**：深入理解文本生成、图像生成、视频生成以及语音合成背后的基本技术原理。
3.  **探索 AI 协同工作流**：探索不同 AI 工具之间的配合模式，建立一套从文案到成片的完整自动化或半自动化制作流程。
4.  **分析 AI 生成内容的优劣**：通过实际操作，评估当前 AI 技术在视频制作中的优势（如效率、创意）与局限性（如一致性、真实感），并提出改进策略。

---

## 二、实验原理

本次实验涉及的核心技术主要包括大语言模型（LLM）、文本生成图像（Text-to-Image）、文本生成视频（Text-to-Video）以及语音合成（Text-to-Speech）。

### 2.1 大语言模型 (Large Language Models, LLM)
实验中使用的“豆包”和“通义千问”均属于大语言模型。LLM 的核心架构通常基于 Transformer 模型，特别是其解码器（Decoder-only）部分（如 GPT 系列）或编码器-解码器（Encoder-Decoder）部分（如 T5 系列）。
Transformer 架构通过**自注意力机制（Self-Attention Mechanism）**来捕捉序列数据中的长距离依赖关系。其核心公式为：
$$ Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
其中，$Q$ (Query), $K$ (Key), $V$ (Value) 分别代表查询向量、键向量和值向量。通过计算 $Q$ 和 $K$ 的点积并进行归一化，模型能够计算出序列中每个词与其他词的相关性权重，从而理解上下文语境。此外，**多头注意力（Multi-Head Attention）**机制允许模型在不同的子空间中并行关注不同的信息特征，极大地增强了模型的表达能力。
LLM 的训练通常分为两个阶段：
1.  **预训练（Pre-training）**：在海量文本数据上进行无监督学习，学习语言的统计规律和世界知识。
2.  **微调（Fine-tuning）**：通过指令微调（Instruction Tuning）和人类反馈强化学习（RLHF），使模型能够更好地遵循人类指令，生成符合人类偏好的回复。
在本次实验中，LLM 主要用于理解原始新闻文本，提取关键信息，并将其转化为用于图像和视频生成的结构化提示词（Prompts），以及生成视频的分镜脚本。

### 2.2 文本生成图像 (Text-to-Image)
实验中用于生成静态素材的技术基础是**扩散模型（Diffusion Models）**，如 Stable Diffusion。扩散模型包含两个过程：
1.  **前向扩散过程（Forward Diffusion Process）**：向真实图像中逐步添加高斯噪声，直到图像完全变成随机噪声。这是一个固定的马尔可夫链过程。
    $$ q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I) $$
2.  **反向去噪过程（Reverse Denoising Process）**：训练一个神经网络（通常是 U-Net 结构）来预测并去除噪声，从而从随机噪声中恢复出清晰的图像。
    $$ p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) $$

为了实现“文本控制生成”，模型通常结合了 **CLIP (Contrastive Language-Image Pre-training)** 模型。CLIP 将文本和图像映射到同一个潜在空间（Latent Space），使得模型能够理解文本描述与视觉内容之间的对应关系。在生成过程中，文本提示词通过 Text Encoder 转化为 Embedding，通过交叉注意力（Cross-Attention）机制注入到 U-Net 的各个层级中，引导去噪方向，从而生成符合文本描述的图像。在实验中，我们输入“英伟达和英特尔握手”的文本，模型便能在潜在空间中寻找对应的视觉特征并生成图像。

### 2.3 文本生成视频 (Text-to-Video)
Sora 2 等视频生成模型是扩散模型在时间维度上的扩展。视频可以被视为一系列连续的图像帧。
1.  **时空注意力机制（Spacetime Attention）**：传统的 U-Net 处理的是 2D 图像，而视频生成模型通常使用 3D U-Net 或基于 Transformer 的架构（如 DiT, Diffusion Transformer），在处理空间特征的同时，通过时间轴上的注意力机制来保持帧与帧之间的连贯性（Temporal Consistency）。
2.  **视频压缩网络（Video Compression Network）**：为了降低计算量，模型通常先将视频压缩到一个低维的潜在空间（Latent Space）中进行扩散过程，生成后再解码回像素空间。
3.  **物理世界模拟**：Sora 等先进模型展现出了对物理规律的一定理解能力（如重力、碰撞、遮挡关系），这得益于其在大规模视频数据上的训练，使其能够“涌现”出世界模型（World Model）的特性。

### 2.4 语音合成 (Text-to-Speech, TTS)
实验中使用的配音技术属于 TTS 范畴。现代 TTS 系统（如 VITS, FastSpeech）通常包含：
1.  **文本前端**：处理文本归一化、分词、韵律预测。
2.  **声学模型**：将文本特征转换为声学特征（如梅尔频谱图）。
3.  **声码器（Vocoder）**：将声学特征转换为波形音频。
为了实现特定角色（如“待兼诗歌剧”）的声音，通常使用**声音克隆（Voice Cloning）**或**声音转换（Voice Conversion）**技术，利用少量的目标说话人音频数据对模型进行微调（Fine-tuning）或零样本适应（Zero-shot Adaptation），从而在保留目标音色特征的同时合成新的语音内容。

---

## 三、实验步骤

本次实验的流程严格遵循 `prompts.md` 中规划的顺序，主要分为素材准备、脚本规划、视频生成、后期剪辑四个阶段。

### 3.1 步骤一：新闻素材分析与提示词设计
首先，我们获取了本次实验的核心文本素材（源自 `要求.txt`）：
> “2025年9月19日凌晨，英伟达创始人兼总裁黄仁勋与英特尔总裁陈立武罕见同屏直播共同宣布了一项震惊全球科技界的合作：英伟达将以50亿美元战略投资英特尔……”

为了将这段文字转化为视频，我们需要将其拆解为可视化的画面。通过分析，我们确定了以下几个关键视觉元素：
1.  **主体人物**：黄仁勋（NVIDIA）与陈立武（Intel）。
2.  **核心事件**：同屏直播、握手、战略投资。
3.  **数据可视化**：50亿美元金额、股价暴涨（K线图）、市值数据。
4.  **象征意义**：商标（Logo）的结合、竞争走向合作。

基于此，我们设计了针对不同 AI 工具的提示词（Prompts）。

### 3.2 步骤二：静态图像素材生成（使用豆包）
我们首先使用“豆包”生成基础的静态图像素材，用于辅助后续的视频生成或直接作为视频中的关键帧。

**输入提示词（Prompt）**：
> 请为文案制作一些图片，nvidia和intel分别用他们的商标图标表示。每个部分制作2-3个图片。
> 具体来说主要分为以下几个主要画面部分：
> 1. nvidia和intel的商标
> 2. 股价上涨画面
> 3. 两家企业合作（在两个商标下画握手）

**执行过程与迭代**：
1.  **初步生成**：将上述 Prompt 输入豆包。豆包基于其对商业概念和视觉符号的理解，生成了多组图片。
    *   *Logo 展示*：生成了带有 NVIDIA 标志性绿色和 Intel 标志性蓝色的科技感背景图。初版图片中，两个 Logo 的比例有时不协调，或者颜色过于饱和。
    *   *股价画面*：生成了包含上升红箭头和 K 线图的金融风格图片。部分图片中尝试加入了“+30%”等文字元素。这里遇到了常见的 AI 生成文字乱码问题，例如将 "30%" 生成为 "300%" 或不可读的符号。
    *   *合作画面*：生成了两个拟人化的 Logo 握手的画面，或者两个 Logo 并排且中间有光束连接的画面。
2.  **筛选与优化**：
    *   针对 Logo 变形的问题，我们尝试在 Prompt 中加入 "official logo, vector style, clean background" 等限定词，强制模型生成更规范的图形。
    *   针对文字乱码问题，我们决定在后续视频生成或剪辑阶段通过贴图覆盖的方式解决，或者选择不包含具体数字的通用 K 线图。
    *   针对“握手”这一抽象概念，我们选择了“机械臂握手”和“光路连接”两种视觉隐喻，分别代表硬件层面的合作和数据层面的打通。
3.  **最终定稿**：经过约 3-4 轮的对话和重新生成（Regenerate），我们筛选出了 7-8 张清晰、构图合理、风格统一的图片，保存在 `images/` 文件夹下，作为后续步骤的基础素材。

### 3.3 步骤三：视频脚本与分镜规划（使用通义千问）
为了让视频叙事更加流畅，我们使用“通义千问”将新闻文本和生成的图片转化为详细的视频时间线脚本。

**输入提示词（Prompt）**：
> (传入生成的图片)
> 请根据画面和下面的提示，按顺序排列图片，并为画面编写视频脚本。
> [附带原始新闻文本...]

**执行过程**：
通义千问展现出了极强的逻辑编排能力。它不仅根据新闻的时间顺序（宣布合作 -> 市场反应 -> 意义升华）排列了图片，还为每一帧画面匹配了具体的解说词（Voiceover）。
生成的脚本结构清晰，将视频分为了以下 7 个主要场景：
1.  **主播开场**：引入新闻背景。
2.  **投资宣布**：展示 50 亿美元投资的核心信息。
3.  **历史回顾**：展示两家公司过去的竞争关系（芯片对峙）。
4.  **握手言和**：展示合作达成的关键时刻。
5.  **技术融合**：展示 Logo 融入芯片阵列，象征技术互通。
6.  **股价反应**：展示 Intel 股价暴涨的 K 线图。
7.  **结尾总结**：主播总结并展望未来。
这个脚本成为了后续 Sora 2 生成视频的直接指导，确保了视频内容的逻辑性和完整性。

### 3.4 步骤四：动态视频生成（使用 Sora 2）
这是实验中最核心也是最耗时的部分。我们依据步骤三生成的脚本，向 Sora 2 发送指令生成视频片段。

**场景 1：新闻主播开场**
*   **挑战**：Sora 2 对于上传真人图片生成视频有限制（出于隐私和安全考虑），且直接生成特定真人（如黄仁勋）可能存在肖像权风险或恐怖谷效应。此外，保持真人在不同镜头中的面部一致性是目前 T2V 模型的难点。
*   **解决方案**：我们决定采用“虚拟主播”方案。我们将主持人形象替换为动漫游戏《赛马娘》中的角色“待兼诗歌剧（曼波）”。这不仅规避了真人生成的限制，也增加了视频的趣味性。
*   **Prompt**：
    > [画面1：新闻主播开场]
    > 【镜头】动漫风格，角色待兼诗歌剧（曼波）正对镜头，作为新闻主播。背景是蓝色科技风格，左侧NVIDIA标志，右侧Intel标志。屏幕下方文字：“全球科技新闻 | 2025年9月19日凌晨”
    > 主播正在播报新闻，嘴唇自然开合，表情严肃而专业。
*   **生成细节**：为了让“曼波”的口型看起来像是在说话，我们在 Prompt 中强调了 "speaking, moving lips, news anchor pose"。虽然 Sora 2 目前还不能完美实现“口型-语音”同步（Lip-sync），但生成的画面中人物有自然的头部晃动和嘴部开合，配合后期的配音，效果足以乱真。

**场景 2-7：新闻内容可视化**
对于后续的画面，我们部分采用了“图生视频”（Image-to-Video），部分采用了“文生视频”（Text-to-Video）。
*   **图生视频**：对于“股价暴涨”和“Logo 握手”等对构图要求较高的场景，我们将步骤二中豆包生成的优质图片作为参考图上传给 Sora 2，让其让图片“动起来”（如让 K 线图的线条延伸、让光效闪烁）。
    *   *技巧*：在 I2V 模式下，我们设置了较高的 `image_strength` 参数（如 0.8），以确保生成的视频忠实于原图的构图，只增加动态细节。
*   **文生视频**：对于“芯片对峙”和“Logo 融入芯片阵列”等动态性较强的场景，直接使用详细的文本描述生成，效果往往比静态图驱动更自然。
    *   *Prompt 示例*：“两枚芯片相对，中间有能量光束交汇，齿轮背景旋转，赛博朋克风格，高分辨率。”
    *   *Prompt 示例*：“绿色NVIDIA标志醒目地置于众多Intel芯片中央，象征‘融入’，镜头缓慢推进（dolly in）。”

**微调（Fine-tuning）**：
在生成过程中，部分视频出现了逻辑错误（如握手动作不自然、Logo 拼写错误）。我们通过修改 Prompt 中的形容词（如增加 "high quality", "detailed", "smooth animation"）或调整随机种子（Seed）进行了多次微调，最终筛选出效果最好的片段保存在 `videos/` 文件夹中。

### 3.5 步骤五：后期剪辑与合成（使用必剪）
最后，我们将所有生成的视频片段和音频素材导入剪辑软件“必剪”进行合成。

1.  **视频拼接**：按照脚本顺序，将 `videos/` 中的片段拖入轨道。
2.  **转场处理**：由于不同 AI 生成的片段在色调和光影上可能存在差异，我们在片段之间添加了“叠化”、“科技感扫描”等转场动画，使视觉过渡更加自然流畅。
3.  **音频制作**：
    *   **配音（TTS）**：利用 TTS 工具，加载“待兼诗歌剧（曼波）”的声音模型（参考 `manbo/clip.ps1` 中的配置），将脚本中的配音文案转化为语音文件。确保语速适中，情感符合新闻播报的基调。
    *   **字幕生成**：使用必剪的“识别字幕”功能，根据生成的语音自动生成字幕，并校对其中的专有名词（如 NVIDIA, Intel, 黄仁勋）。
    *   **背景音乐**：添加了一段轻快且带有科技感的背景音乐（BGM），音量调低，以免掩盖人声。
4.  **导出**：检查音画同步情况，最终导出为 MP4 格式的成片。

---

## 四、实验结果

经过上述步骤，我们成功制作出了一段时长约 1 分 30 秒的 AI 新闻短视频。

### 4.1 视频内容概述
*   **开头**：虚拟主播“曼波”在科技感演播室背景下开场，口型与语音基本同步，播报了英伟达投资英特尔的重磅新闻。
*   **中间段落**：
    *   画面流畅地展示了 NVIDIA 和 Intel 的 Logo 动态特效，视觉冲击力强。
    *   “芯片握手”的画面极具象征意义，生动地传达了“合作”的主题。
    *   股价上涨的 K 线图动画清晰直观，配合红色的上涨箭头，营造出市场利好的氛围。
*   **结尾**：主播回到画面进行总结，并预告后续关注。

### 4.2 质量评估
*   **视觉效果**：Sora 2 生成的视频片段在光影处理和纹理细节上表现出色，尤其是芯片的金属质感和电路板的流动光效，达到了准专业级的视觉水准。
*   **连贯性**：通过脚本的精心编排和转场的运用，视频整体叙事连贯，逻辑清晰。
*   **音频效果**：使用特定角色（曼波）的音色使得视频别具一格，打破了传统新闻播报的沉闷感，TTS 的清晰度和情感表现力也令人满意。

### 4.3 存在的问题
尽管整体效果良好，但仍存在一些瑕疵：
*   **文字渲染**：视频中出现的文字（如新闻标题、股价数字）偶尔会出现拼写错误或笔画扭曲的现象，这是当前视频生成模型的通病。
*   **物理逻辑**：在“芯片握手”的画面中，机械臂的关节运动偶尔显得生硬，不完全符合物理规律。
*   **风格统一性**：虽然使用了转场，但不同 Prompt 生成的片段在画风上（如写实度、色彩饱和度）仍有细微差别，影响了整体的统一感。

---

## 五、实验总结与分析

### 5.1 实验总结
本次实验成功验证了利用 AI 工具全流程制作短视频的可行性。从文案理解到视觉呈现，再到最终成片，AI 展现出了强大的辅助能力。
1.  **效率提升**：传统制作流程中需要数天甚至数周的建模、渲染、拍摄工作，通过 AI 工具压缩到了数小时内完成。
2.  **创意落地**：AI 能够快速将抽象的概念（如“竞争走向绑定”、“芯片握手”）转化为具体的视觉图像，极大地拓展了创意的边界。
3.  **门槛降低**：即使不具备专业的绘画或动画制作技能，也能通过自然语言描述制作出高质量的视频内容。

### 5.2 分析与思考
**1. 提示词工程（Prompt Engineering）的重要性**
实验发现，生成质量的高低高度依赖于提示词的质量。一个好的提示词不仅需要描述画面内容，还需要包含风格（Style）、光影（Lighting）、镜头语言（Camera Angle）、画质要求（Quality）等详细参数。例如，加上 "cinematic lighting, 8k resolution, unreal engine 5 render" 等词汇，生成的画面质感会有显著提升。此外，**负向提示词（Negative Prompt）**的使用也至关重要，如 "bad anatomy, blurry, text error, distorted logo" 等，可以有效减少生成瑕疵。

**2. 多模态模型的局限性与未来**
目前的 AI 视频生成仍处于“抽卡”阶段（即生成结果具有随机性，需要多次尝试）。模型对于复杂的时空逻辑、精确的文本渲染以及长视频的一致性保持仍有待提高。
*   **一致性问题**：在不同镜头中保持同一角色（如主播）的长相、服饰完全一致是非常困难的。本次实验通过使用动漫角色和固定参考图在一定程度上缓解了这个问题，但并未根除。未来的模型可能会引入 Character LoRA (Low-Rank Adaptation) 或 ControlNet 等技术来增强对特定角色的控制力。
*   **可控性问题**：用户很难精确控制视频中物体的运动轨迹（如“让这辆车以 60km/h 的速度向左转 30 度”）。这需要模型具备更强的物理世界理解能力。

**3. 伦理与法律考量**
随着 AI 生成内容的逼真度越来越高，Deepfake（深度伪造）带来的伦理风险不容忽视。
*   **虚假新闻**：本次实验虽然明确标注为虚构新闻，但如果此类技术被用于制造逼真的假新闻，可能会对社会造成严重误导。因此，建立完善的 AI 内容标识机制（如数字水印）刻不容缓。
*   **版权问题**：AI 模型的训练数据往往来源于互联网，生成的图像或视频是否侵犯了原艺术家的版权，目前在法律上仍存在争议。我们在使用 AI 工具时，应尊重原创，避免直接生成特定画师风格的作品用于商业用途。

**4. AI 与人的关系**
AI 在本实验中扮演的是“执行者”和“素材库”的角色，而“导演”和“剪辑师”的角色仍由人来承担。人需要负责核心的创意构思、脚本把控、审美筛选以及情感注入。AI 不会取代创作者，而是成为创作者手中最强大的工具，将创作者从繁琐的重复性劳动中解放出来，去专注于更高维度的创意表达。

综上所述，本次实验不仅让我们掌握了具体的 AI 工具操作技能，更让我们深刻体会到了 AIGC 时代的到来。未来，随着技术的进一步迭代，AI 视频生成的质量和可控性将大幅提升，必将彻底改变媒体、娱乐、教育等行业的生产范式。
