# 实验一 一维PCA和二维PCA，识别特征脸

## 一、实验目的

- 理解主成分分析（PCA）和二维主成分分析（2D-PCA）的基本思想与数学原理，掌握其在降维与特征提取中的作用。
- 在人脸识别任务中，学习如何利用一维PCA构建“特征脸（Eigenfaces）”并进行分类识别。
- 理解2D-PCA相对于传统PCA在图像表示上的改进思路，比较两种方法在特征表达和识别效果上的差异。
- 熟悉在 Python 环境下使用 `numpy`、`matplotlib` 等库进行矩阵运算、特征分解与可视化的基本流程。

## 二、实验原理

### 1. 一维PCA与特征脸

PCA 的目标是在保持数据主要变化信息的前提下，将高维数据投影到低维子空间。设原始样本为 $x_i \in \mathbb{R}^d$，PCA 的基本步骤：

1. 计算样本均值向量 $\mu = \frac{1}{N}\sum_{i=1}^N x_i$，对样本做中心化处理：$\tilde{x}_i = x_i - \mu$。
2. 计算协方差矩阵 $S = \frac{1}{N}\sum_{i=1}^N \tilde{x}_i \tilde{x}_i^T$。
3. 对 $S$ 做特征值分解，得到特征值 $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_d$ 及对应特征向量 $u_1, u_2, \dots, u_d$。
4. 取前 $k$ 个最大特征值对应的特征向量构成投影矩阵 $U_k = [u_1, u_2, \dots, u_k]$，将样本映射到低维空间：
	$$y_i = U_k^T (x_i - \mu).$$

在“特征脸”方法中，我们将每张人脸图像按行优先方式拉平成一维向量作为 $x_i$，得到的特征向量 $u_j$ 再按图像尺寸重排后就是一张“特征脸”。特征脸在视觉上体现了数据集中最具有代表性的变化模式。识别时，将待测人脸投影到特征脸子空间中，与训练集中已知类别的投影向量做距离度量（如欧式距离），根据最近邻原则完成分类。

### 2. 二维PCA（2D-PCA）

传统PCA需要将图像矩阵展平成长向量，不仅维度高、计算协方差矩阵开销大，而且破坏了图像的二维结构信息。2D-PCA 直接在图像矩阵上进行特征提取。设每个样本图像为矩阵 $A_i \in \mathbb{R}^{m\times n}$，2D-PCA 的基本思想是寻找一个投影矩阵 $W \in \mathbb{R}^{n\times d}$，使得投影后的特征矩阵 $Y_i = A_i W$ 能最大程度保留样本之间的散布：

1. 计算样本均值矩阵 $\bar{A} = \frac{1}{N}\sum_{i=1}^N A_i$。
2. 构造图像协方差矩阵：
	$$G = \frac{1}{N}\sum_{i=1}^N (A_i - \bar{A})^T (A_i - \bar{A}).$$
3. 对矩阵 $G$ 进行特征值分解，选取最大若干特征值对应的特征向量构成投影矩阵 $W$。
4. 对每个样本，计算 $Y_i = A_i W$ 作为其二维特征表示，后续再将 $Y_i$ 展平或使用矩阵距离进行分类。

与一维PCA相比，2D-PCA 直接在矩阵空间上建模，协方差矩阵维度更低，计算量更小，而且保留了图像行方向的结构信息，通常在样本数有限时表现出更稳定的特征提取能力。

### 3. 特征脸识别流程概述

综合以上原理，本实验中的特征脸识别过程可概括为：

1. 数据预处理：读取人脸图像，统一尺寸并灰度化，将像素值标准化到 [0,1] 或 [0,255] 范围。
2. 训练阶段：
	- 计算训练样本的人脸均值图像；
	- 计算协方差矩阵并进行特征分解；
	- 选取前 $k$ 个主成分，得到特征脸集合；
	- 将所有训练样本投影到特征空间，保存其特征表示及标签。
3. 测试识别：
	- 将待测人脸做同样的预处理和投影；
	- 计算其与训练样本特征向量之间的距离；
	- 选取距离最近的类别作为识别结果。

## 三、实验环境与数据说明

- 硬件环境：
  - CPU：常规多核处理器即可完成实验；
  - 内存：≥ 8 GB。
- 软件环境：
  - 操作系统：Windows 10；
  - 编程语言：Python 3（使用项目根目录的 `pyproject.toml` 中配置的环境）；
  - 主要第三方库：`numpy`、`matplotlib` 等。
- 开发工具：Visual Studio Code，配合 Jupyter Notebook 进行实验与可视化。
- 数据说明：
  - 实验数据存放于项目根目录的 `data/olivetti_py3.pkz` 文件中，基于 Olivetti faces 数据集预处理而来；
  - 每个样本是一张尺寸固定的人脸灰度图像，标签为对应的人物编号；
  - 在 Notebook `expr1/pca.ipynb` 中用于一维PCA与特征脸实验，在 `expr1/2d-pca.ipynb` 中用于2D-PCA实验。

## 四、实验步骤与结果

### 1. 一维PCA与特征脸

1. **加载数据与可视化样本**：
	- 使用 `numpy` 从 `olivetti_py3.pkz` 中读取人脸图像矩阵和对应标签；
	- 随机选取若干张人脸图像，使用 `matplotlib` 绘制，观察数据的大致分布和姿态变化。

2. **构建训练集与测试集**：
	- 按人物编号划分训练和测试样本，例如每个身份选取若干张用于训练，其余用于测试；
	- 将每张图像拉平成向量，构成训练数据矩阵 $X \in \mathbb{R}^{d \times N}$。

3. **计算均值脸与特征脸**：
	- 计算所有训练样本的平均向量并重塑为图像形式，得到“均值脸”，可以使用 `imshow` 绘制；
	- 基于中心化后的训练数据计算协方差矩阵并进行特征值分解；
	- 选取前 $k$ 个主成分，将其重构为图像形式并可视化，得到一组具有代表性纹理和结构的特征脸。

4. **投影与重构实验**：
	- 将训练和测试样本投影到特征脸子空间，得到低维特征向量；
	- 通过保留不同数量的主成分（例如 $k=10, 20, 50$），对样本进行重构，观察重构图像与原图之间的差异，分析信息损失情况。

5. **人脸识别实验**：
	- 在特征空间中使用简单的最近邻分类器，对测试集人脸进行识别；
	- 统计识别准确率，并观察若干识别正确与识别错误的样例图像，分析误差来源。

整体上，在 `pca.ipynb` 中可以看到：随着保留主成分数量 $k$ 的增加，重构图像质量逐步提升，识别率也有一定改善；但当 $k$ 过大时会带来过拟合风险和计算开销增加。

### 2. 二维PCA（2D-PCA）

1. **直接在图像矩阵上建模**：
	- 不再将图像展平为长向量，而是保持原始矩阵形状 $m \times n$；
	- 计算均值图像矩阵 $\bar{A}$，并据此构造 2D-PCA 的协方差矩阵 $G$。

2. **求取投影矩阵并提取特征**：
	- 对 $G$ 进行特征值分解，选取前 $d$ 个特征向量组成投影矩阵 $W$；
	- 对每个样本计算 $Y_i = A_i W$，得到尺寸为 $m \times d$ 的特征矩阵。

3. **可视化与重构观察**：
	- 对若干样本的二维特征进行可视化（例如展示特征矩阵的灰度图或主方向）；
	- 将特征矩阵再映射回近似原图像（如果在 Notebook 中实现了相应过程），比较重构效果。

4. **2D-PCA 人脸识别**：
	- 将 $Y_i$ 展平为特征向量，使用同样的最近邻方法进行分类；
	- 记录 2D-PCA 在相同训练/测试划分下的识别准确率，与一维PCA结果对比。

实验现象表明：在相近特征维度下，2D-PCA 通常能够在较少主成分数量时获得与 PCA 相当或略优的识别效果，同时计算协方差矩阵与特征分解的时间开销更小。

## 五、实验结果分析

1. **降维效果与信息保留**：
	- 一维PCA通过特征值大小描述每个主成分对整体方差的贡献，可通过累计贡献率曲线选择合适的主成分数 $k$；
	- 在实验中，前几十个主成分往往已经可以重构出较为清晰的人脸轮廓，说明人脸数据在高维空间中具有较强的低维结构。

2. **PCA 与 2D-PCA 的对比**：
	- 传统PCA需要在非常高维的空间中计算协方差矩阵，而样本数量有限时容易出现协方差矩阵病态、特征估计不稳定等问题；
	- 2D-PCA 直接基于图像矩阵构造协方差，矩阵维度显著降低，更适合样本数与图像维度同级甚至更小时的场景；
	- 在本实验中，2D-PCA 在相同特征维度下通常取得了与 PCA 相近甚至更高的识别率，同时计算时间更短、数值更稳定。

3. **识别误差原因分析**：
	- 低维特征维度过小会导致关键信息丢失，重构图像模糊、个体差异减弱，进而降低识别准确率；
	- 数据集中存在姿态变化、光照变化、表情变化等因素，这些变化在主成分空间中有时会被解释为“主要变化”，从而干扰身份特征；
	- 使用最近邻分类时，对噪声和类间边界分布较为敏感，如训练样本不足或分布不均衡，也会造成误识；
	- 仅使用线性投影的 PCA/2D-PCA 难以刻画高度非线性的特征结构，限制了上限性能。

4. **与论文结论的对照**：
	- 参考《Eigenfaces for Recognition》中提出的特征脸方法，本实验复现了“利用少量主成分即可实现较高识别率”的基本结论；
	- 对比《Two-dimensional PCA: A new approach to appearance-based face representation and recognition》中对2D-PCA的分析，实验也体现出 2D-PCA 在图像表示上的紧凑性和鲁棒性优势。

## 六、心得体会

通过本次实验，我对 PCA 和 2D-PCA 在人脸识别中的应用有了更加直观的理解。一方面，从实现角度体会到：

- 仅通过 `numpy` 的基础线性代数运算，就可以完成协方差矩阵的构造与特征值分解，说明许多经典模式识别算法的核心实现并不复杂；
- 在 Notebook 中将均值脸、特征脸以及重构结果可视化，可以非常直观地感受到“降维”到底丢掉了哪些信息，有助于理解主成分的物理意义；
- 通过调节主成分个数、训练测试划分等超参数，可以清楚地看到模型容量、过拟合与泛化性能之间的权衡关系。

另一方面，从方法比较的角度：

- 2D-PCA 在保持图像二维结构、降低协方差矩阵维度方面的优势非常明显，在小样本场景下更容易得到稳定的特征；
- 实验中也发现，单纯依赖线性投影的 PCA/2D-PCA 虽然实现简单，但在复杂光照、姿态变化面前能力有限，如果要在真实应用中获得更高的识别率，还需要结合后续更高级的方法（例如 Fisher 判别分析、核方法或深度学习特征）。

总体而言，本实验让我更加清楚地认识到：数学上的线性代数和概率统计知识是理解多媒体分析与模式识别算法的基础；通过亲手实现和调参，可以把抽象的公式和真实的数据联系起来，为后续学习更复杂的特征学习和深度学习模型打下良好基础。
